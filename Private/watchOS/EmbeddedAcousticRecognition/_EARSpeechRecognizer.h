//
//   Generated by https://github.com/blacktop/ipsw (Version: 3.1.454, BuildTime: 2024-02-08T22:07:34Z)
//
//    - LC_BUILD_VERSION:  Platform: watchOSSimulator, MinOS: 10.2, SDK: 10.2, Tool: ld (902.8)
//    - LC_SOURCE_VERSION: 3302.7.1.0.0
//
#ifndef _EARSpeechRecognizer_h
#define _EARSpeechRecognizer_h
@import Foundation;

#include "EARVoiceCommandActiveSet.h"
#include "_EARFormatter.h"
#include "_EARRecognitionMetrics.h"
#include "_EARSpeakerCodeInfo.h"
#include "_EARSpeechModelInfo.h"
#include "_EARSpeechRecognitionAudioBuffer.h"
#include "_EARTokenizer.h"

@class NSArray, NSData, NSDictionary, NSNumber, NSSet, NSString;
@protocol OS_dispatch_queue, {shared_ptr<EARModelInitializeContext>="__ptr_"^{EARModelInitializeContext}"__cntrl_"^{__shared_weak_count}}, {shared_ptr<const quasar::VoiceCommandActiveSetCompilation>="__ptr_"^{VoiceCommandActiveSetCompilation}"__cntrl_"^{__shared_weak_count}}, {shared_ptr<quasar::SpeakerCodeTraining>="__ptr_"^{SpeakerCodeTraining}"__cntrl_"^{__shared_weak_count}}, {shared_ptr<quasar::SpeechRecognizer>="__ptr_"^{SpeechRecognizer}"__cntrl_"^{__shared_weak_count}}, {vector<std::string, std::allocator<std::string>>="__begin_"^v"__end_"^v"__end_cap_"{__compressed_pair<std::string *, std::allocator<std::string>>="__value_"^v}}, {weak_ptr<ResultStreamWrapper>="__ptr_"^{ResultStreamWrapper}"__cntrl_"^{__shared_weak_count}};

@interface _EARSpeechRecognizer : NSObject {
  /* instance variables */
  NSObject<OS_dispatch_queue> *_formatterQueue;
  _EARFormatter *_formatter;
  NSObject<OS_dispatch_queue> *_trainingQueue;
  struct shared_ptr<quasar::SpeakerCodeTraining> { struct SpeakerCodeTraining *__ptr_; struct __shared_weak_count *__cntrl_; } _training;
  struct shared_ptr<const quasar::VoiceCommandActiveSetCompilation> { struct VoiceCommandActiveSetCompilation *__ptr_; struct __shared_weak_count *__cntrl_; } _voiceCommandCompilation;
  NSSet *_endsOfSentencePunctuations;
  struct shared_ptr<quasar::SpeechRecognizer> { struct SpeechRecognizer *__ptr_; struct __shared_weak_count *__cntrl_; } _recognizer;
  _EARSpeechRecognitionAudioBuffer *_currentAudioBuffer;
  struct weak_ptr<ResultStreamWrapper> { struct ResultStreamWrapper *__ptr_; struct __shared_weak_count *__cntrl_; } _currentResultStreamWrapper;
  NSString *_currentLanguage;
  NSString *_currentTask;
  unsigned long long _currentSamplingRate;
  NSObject<OS_dispatch_queue> *_recognitionQueue;
  NSDictionary *_muxIdMask;
  NSDictionary *_muxIdReverseMask;
  NSSet *_muxIds;
  NSArray *_userProfiles;
  struct vector<std::string, std::allocator<std::string>> { void *__begin_; void *__end_; struct __compressed_pair<std::string *, std::allocator<std::string>> { void *__value_; } __end_cap_; } _rightContextTokens;
  struct shared_ptr<EARModelInitializeContext> { struct EARModelInitializeContext *__ptr_; struct __shared_weak_count *__cntrl_; } _modelInitializeContext;
  NSArray *_onScreenContextForEditLme;
  BOOL _loadLmeForVoiceCommand;
  _EARTokenizer *_tokenizer;
}

@property (readonly, nonatomic) BOOL isContinuousListening;
@property (readonly, nonatomic) NSString *configPath;
@property (readonly, nonatomic) unsigned short itnEnablingFlags;
@property (retain, nonatomic) NSNumber *overrideDoServerSideEndpointing;
@property (copy, nonatomic) NSData *userProfileData;
@property (copy, nonatomic) NSData *jitProfileData;
@property (readonly, nonatomic) _EARSpeechModelInfo *modelInfo;
@property (readonly, nonatomic) _EARSpeakerCodeInfo *speakerCodeInfo;
@property (nonatomic) BOOL detectUtterances;
@property (nonatomic) BOOL concatenateUtterances;
@property (nonatomic) BOOL allowUtteranceDelay;
@property (nonatomic) BOOL formatAcrossUtterances;
@property (nonatomic) double endpointStart;
@property (nonatomic) BOOL recognizeEagerCandidates;
@property (nonatomic) BOOL farField;
@property (nonatomic) BOOL highPriority;
@property (nonatomic) BOOL enableSpeakerCodeTraining;
@property (nonatomic) double maximumRecognitionDuration;
@property (copy, nonatomic) NSDictionary *recognitionReplacements;
@property (copy, nonatomic) NSDictionary *recognitionConfidenceSubtraction;
@property (copy, nonatomic) NSArray *leftContext;
@property (copy, nonatomic) NSString *inputOrigin;
@property (copy, nonatomic) NSString *deviceId;
@property (copy, nonatomic) NSString *refTranscriptForErrorBlaming;
@property (copy, nonatomic) NSString *bluetoothDeviceId;
@property (copy, nonatomic) NSString *userId;
@property (copy, nonatomic) NSString *sessionId;
@property (copy, nonatomic) NSArray *extraLmList;
@property (copy, nonatomic) NSArray *scoreNbestExtraLmList;
@property (nonatomic) BOOL scoreNbest;
@property (nonatomic) double latitude;
@property (nonatomic) double longitude;
@property (nonatomic) BOOL disableAutoPunctuation;
@property (nonatomic) BOOL disablePartialResults;
@property (nonatomic) BOOL enableVoiceCommands;
@property (nonatomic) BOOL shouldGenerateVoiceCommandCandidates;
@property (readonly, nonatomic) EARVoiceCommandActiveSet *voiceCommandActiveSet;
@property (nonatomic) BOOL recognizeEmoji;
@property (copy, nonatomic) NSString *rightContext;
@property (copy, nonatomic) NSString *selectedText;
@property (copy, nonatomic) NSString *aneContext;
@property (copy, nonatomic) NSString *cpuContext;
@property (copy, nonatomic) NSString *gpuContext;
@property (copy, nonatomic) _EARRecognitionMetrics *recognitionMetrics;
@property (copy, nonatomic) NSArray *leftContextForItn;

/* class methods */
+ (void)initialize;
+ (id)minimumSupportedConfigurationVersion;
+ (id)maximumSupportedConfigurationVersion;
+ (id)rawTokenResultsFromRecognitionResults:(id)results;
+ (void)compileRecognizerModelsWithConfiguration:(id)configuration;
+ (void)purgeCompiledRecognizerModelsWithConfiguration:(id)configuration;

/* instance methods */
- (id)initWithConfiguration:(id)configuration;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides;
- (id)initWithConfiguration:(id)configuration overrideConfigFiles:(id)files;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration enableSpeakerCodeTraining:(BOOL)training;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition voiceCommandActiveSet:(id)set;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition voiceCommandActiveSet:(id)set modelContextDelegate:(id)delegate;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition voiceCommandActiveSet:(id)set modelContextDelegate:(id)delegate enableItn:(BOOL)itn;
- (id)initWithConfiguration:(id)configuration withLanguage:(id)language withSdapiConfig:(id)config;
- (id)initWithConfiguration:(id)configuration withGeneralVoc:(id)voc withLexiconEnh:(id)enh withItnEnh:(id)enh;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh;
- (id)initWithConfiguration:(id)configuration overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration enableSpeakerCodeTraining:(BOOL)training;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition voiceCommandActiveSet:(id)set;
- (id)initWithConfiguration:(id)configuration overrides:(id)overrides overrideConfigFiles:(id)files generalVoc:(id)voc lexiconEnh:(id)enh itnEnh:(id)enh language:(id)language activeConfiguration:(id)configuration modelLoadingOptions:(id)options enableSpeakerCodeTraining:(BOOL)training supportEmojiRecognition:(BOOL)recognition voiceCommandActiveSet:(id)set modelContextDelegate:(id)delegate;
- (id)initWithConfiguration:(id)configuration useQuasarFormatter:(BOOL)formatter;
- (id)initWithConfiguration:(id)configuration useQuasarFormatter:(BOOL)formatter activeConfiguration:(id)configuration;
- (id)_tokenizer;
- (void)setLeftContextText:(id)text;
- (void)_setProfileContainers:(id)containers muxIds:(id)ids;
- (void)setUserProfile:(id)profile;
- (id)_unmaskMuxPackages:(id)packages;
- (id)runRecognitionWithResultStream:(id)stream;
- (void)updateUserProfileData:(id)data;
- (void)updateJitProfileData:(id)data;
- (id)runRecognitionWithResultStream:(id)stream language:(id)language task:(id)task samplingRate:(unsigned long long)rate;
- (id)runRecognitionWithResultStream:(id)stream speakerCodeWriter:(id)writer language:(id)language task:(id)task samplingRate:(unsigned long long)rate;
- (id)runRecognitionWithResultStream:(id)stream language:(id)language task:(id)task samplingRate:(unsigned long long)rate userProfileData:(id)data speakerCodeWriter:(id)writer;
- (BOOL)canCloneIsFinalAsLastNonFinal;
- (void)writeRecordedStateAccesses;
- (struct shared_ptr<quasar::RecogAudioBufferBase> { struct RecogAudioBufferBase * x0; struct __shared_weak_count * x1; })_audioBufferWithLangauge:(id)langauge task:(id)task samplingRate:(unsigned long long)rate userProfileData:(id)data resultStream:(struct shared_ptr<quasar::RecogResultStreamBase> { struct RecogResultStreamBase * x0; struct __shared_weak_count * x1; })stream;
- (void)_restartActiveRecognition;
- (id)recognitionResultsWithAudioData:(id)data userProfileData:(id)data language:(id)language task:(id)task samplingRate:(unsigned long long)rate;
- (id)recognitionResultsWithAudioData:(id)data userProfileData:(id)data language:(id)language task:(id)task samplingRate:(unsigned long long)rate extraLanguageModel:(id)model;
- (id)testFormattingWithOneBestResults:(id)results uttMillis:(id)millis;
- (void)cancelRecognition;
- (void)_waitForAsyncRecogToFinish;
- (void)interruptTraining;
- (id)recognitionStatistics;
- (id)recognitionUtterenceStatistics;
- (id)recognitionUtteranceInfos;
- (void)getFormatterWithBlock:(id /* block */)block;
- (void)_waitForInitialization;
- (void)dumpModelVirtualMemoryInfo;
- (void)setActiveConfiguration:(id)configuration;
- (BOOL)isSpeakerCodeTrainingSupported:(id)supported;
- (id)activeConfiguration;
- (void)setAlternateRawRecognitionTokenSausage:(id)sausage;
- (struct shared_ptr<quasar::SpeechRecognizer> { struct SpeechRecognizer * x0; struct __shared_weak_count * x1; })getRecognizer;
- (void)requestEagerResult:(id)result;
- (int)getCachedTokensSize;
- (void)pauseRecognition;
- (void)resumeRecognitionWithLeftContext:(id)context rightContext:(id)context selectedText:(id)text;
- (id)tokenizeTextFromEnd:(id)end withLimit:(unsigned long long)limit outTokensInVocab:(id *)vocab;
- (struct vector<std::string, std::allocator<std::string>> { void * x0; void * x1; struct __compressed_pair<std::string *, std::allocator<std::string>> { void * x0; } x2; })splitWithTokenizer:(id)tokenizer outTokensInVocab:(id *)vocab isLeftContext:(BOOL)context;
- (struct vector<std::string, std::allocator<std::string>> { void * x0; void * x1; struct __compressed_pair<std::string *, std::allocator<std::string>> { void * x0; } x2; })splitWithTokenizer:(id)tokenizer isLeftContext:(BOOL)context shouldTruncate:(BOOL)truncate outTokensInVocab:(id *)vocab;
@end

#endif /* _EARSpeechRecognizer_h */
